{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import load_from_disk\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 값 분석 - Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ans(txt):\n",
    "    return txt['text'][0]\n",
    "def valid_result(dataset_path, pred_path, print_result, return_result):\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    valid_dataset = dataset[\"validation\"]\n",
    "    valid_df=pd.DataFrame(valid_dataset)\n",
    "    valid_df['con_len'] = valid_df['context'].apply(lambda x: len(x))\n",
    "    valid_df['qu_len'] = valid_df['question'].apply(lambda x: len(x))\n",
    "    valid_df['ans_len']=[len(valid_df['answers'][i]['text'][0]) for i in range(len(valid_df))]\n",
    "\n",
    "    valid_df['answer_text'] = valid_df[\"answers\"].apply(find_ans)\n",
    "\n",
    "    with open(pred_path) as f:\n",
    "        prediction = json.load(f)\n",
    "    pred_id = []\n",
    "    pred_ans = []\n",
    "    for k,v in enumerate(prediction):\n",
    "        pred_id.append(v)\n",
    "        pred_ans.append(prediction[v])\n",
    "    pred_df = pd.DataFrame({'id': pred_id, 'pred_ans': pred_ans})\n",
    "\n",
    "    valid_pred_df = pd.merge(valid_df, pred_df, on='id')\n",
    "    valid_pred_df = valid_pred_df[['title','context','question','answer_text','pred_ans','con_len','qu_len','ans_len']]\n",
    "    corect_df = valid_pred_df.loc[valid_pred_df['answer_text']==valid_pred_df['pred_ans']]\n",
    "    incorect_df = valid_pred_df.loc[valid_pred_df['answer_text']!=valid_pred_df['pred_ans']]\n",
    "    \n",
    "    if print_result:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=1)\n",
    "\n",
    "        plt.subplot(3,2,1)\n",
    "        plt.plot(sorted(corect_df[\"con_len\"]))\n",
    "        plt.xlabel(\"Data index\")\n",
    "        plt.ylabel(\"Length of Data\")\n",
    "        plt.title(\"Length of Correct Context Data\")\n",
    "\n",
    "        plt.subplot(3,2,2)\n",
    "        plt.plot(sorted(incorect_df[\"con_len\"]))\n",
    "        plt.xlabel(\"Data index\")\n",
    "        plt.ylabel(\"Length of Data\")\n",
    "        plt.title(\"Length of Incorrect Context Data\")\n",
    "\n",
    "        plt.subplot(3,2,3)\n",
    "        plt.plot(sorted(corect_df[\"qu_len\"]))\n",
    "        plt.xlabel(\"Data index\")\n",
    "        plt.ylabel(\"Length of Data\")\n",
    "        plt.title(\"Length of Correct Question Data\")\n",
    "\n",
    "        plt.subplot(3,2,4)\n",
    "        plt.plot(sorted(incorect_df[\"qu_len\"]))\n",
    "        plt.xlabel(\"Data index\")\n",
    "        plt.ylabel(\"Length of Data\")\n",
    "        plt.title(\"Length of Incorrect Question Data\")\n",
    "\n",
    "        plt.subplot(3,2,5)\n",
    "        plt.plot(sorted(corect_df[\"ans_len\"]))\n",
    "        plt.xlabel(\"Data index\")\n",
    "        plt.ylabel(\"Length of Data\")\n",
    "        plt.title(\"Length of Correct Answer Data\")\n",
    "\n",
    "        plt.subplot(3,2,6)\n",
    "        plt.plot(sorted(incorect_df[\"ans_len\"]))\n",
    "        plt.xlabel(\"Data index\")\n",
    "        plt.ylabel(\"Length of Data\")\n",
    "        plt.title(\"Length of Incorrect Answer Data\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=1)\n",
    "\n",
    "        plt.subplot(3,2,1)\n",
    "        plt.hist(sorted(corect_df[\"con_len\"]))\n",
    "        plt.xlabel(\"Length\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Frequency of Correct Context Data\")\n",
    "\n",
    "        plt.subplot(3,2,2)\n",
    "        plt.hist(sorted(incorect_df[\"con_len\"]))\n",
    "        plt.xlabel(\"Length\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Frequency of Incorrect Context Data\")\n",
    "\n",
    "        plt.subplot(3,2,3)\n",
    "        plt.hist(sorted(corect_df[\"qu_len\"]))\n",
    "        plt.xlabel(\"Length\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Frequency of Correct Question Data\")\n",
    "\n",
    "        plt.subplot(3,2,4)\n",
    "        plt.hist(sorted(incorect_df[\"qu_len\"]))\n",
    "        plt.xlabel(\"Length\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Frequency of Incorrect Question Data\")\n",
    "\n",
    "        plt.subplot(3,2,5)\n",
    "        plt.hist(sorted(corect_df[\"ans_len\"]))\n",
    "        plt.xlabel(\"Length\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Frequency of Correct Answer Data\")\n",
    "\n",
    "        plt.subplot(3,2,6)\n",
    "        plt.hist(sorted(incorect_df[\"ans_len\"]))\n",
    "        plt.xlabel(\"Length\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Frequency of Incorrect Answer Data\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print('맞춘 개수: ', len(valid_pred_df)-len(incorect_df))\n",
    "        print(corect_df[[\"con_len\",\"qu_len\",\"ans_len\"]].describe())\n",
    "        print('틀린 개수: ', len(incorect_df))\n",
    "        print(incorect_df[[\"con_len\",\"qu_len\",\"ans_len\"]].describe())\n",
    "    if return_result:\n",
    "        return corect_df, incorect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corect_df, incorect_df = valid_result(dataset_path=\"/opt/ml/input/data/train_dataset\",\n",
    "                                      pred_path = \"/opt/ml/input/code/outputs/train_dataset/predictions.json\", \n",
    "                                      print_result=True, return_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corect_df, incorect_df = valid_result(dataset_path=\"/opt/ml/input/data/data_wiki_korquad\",\n",
    "                                      pred_path = \"/opt/ml/input/code/outputs/data_wiki_korquad/predictions.json\", \n",
    "                                      print_result=True, return_result=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 값 분석 - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import shap\n",
    "import torch\n",
    "\n",
    "# load the model\n",
    "pmodel = transformers.pipeline('question-answering')\n",
    "\n",
    "# define two predictions, one that outputs the logits for the range start,\n",
    "# and the other for the range end\n",
    "def f(questions, start):\n",
    "    outs = []\n",
    "    for q in questions:\n",
    "        question, context = q.split(\"[SEP]\")\n",
    "        d = pmodel.tokenizer(question, context)\n",
    "        out = pmodel.model.forward(**{k: torch.tensor(d[k]).reshape(1, -1) for k in d})\n",
    "        logits = out.start_logits if start else out.end_logits\n",
    "        outs.append(logits.reshape(-1).detach().numpy())\n",
    "    return outs\n",
    "def f_start(questions):\n",
    "    return f(questions, True)\n",
    "def f_end(questions):\n",
    "    return f(questions, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"What is on the table?[SEP]When I got home today I saw my cat on the table, and my frog on the floor.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_answer_scorer(answers):\n",
    "    def f(questions):\n",
    "        out = []\n",
    "        for q in questions:\n",
    "            question, context = q.split(\"[SEP]\")\n",
    "            results = pmodel(question, context, topk=20)\n",
    "            values = []\n",
    "            for answer in answers:\n",
    "                value = 0\n",
    "                for result in results:\n",
    "                    if result[\"answer\"] == answer:\n",
    "                        value = result[\"score\"]\n",
    "                        break\n",
    "                values.append(value)\n",
    "            out.append(values)\n",
    "        return out\n",
    "    f.output_names = answers\n",
    "    return f\n",
    "\n",
    "f_answers = make_answer_scorer([\"my cat\", \"cat\", \"my frog\"])\n",
    "explainer_answers = shap.Explainer(f_answers, pmodel.tokenizer)\n",
    "shap_values_answers = explainer_answers(data)\n",
    "\n",
    "shap.plots.text(shap_values_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from datamodule.base_data import *\n",
    "from utils.data_utils import *\n",
    "from utils.util import *\n",
    "from omegaconf import OmegaConf\n",
    "from models.base_model import *\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, default=\"base_config\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "cfg = OmegaConf.load(f\"/opt/ml/input/code/pl/config/{args.config}.yaml\")\n",
    "pl.seed_everything(cfg.train.seed, workers=True)\n",
    "\n",
    "\n",
    "# dataloader와 model을 생성합니다.\n",
    "dataloader = Dataloader(\n",
    "    cfg.model.model_name,\n",
    "    cfg.train.batch_size,\n",
    "    cfg.data.shuffle,\n",
    "    cfg.path.train_path,\n",
    "    cfg.path.test_path,\n",
    "    cfg.train.seed,\n",
    "    cfg.retrieval,\n",
    ")\n",
    "\n",
    "# ckpt_path = \"/opt/ml/input/code/pl/output/klue_roberta-large/epoch=3_val_em=70.00_korquad.ckpt\"\n",
    "pt_path = \"/opt/ml/input/code/pl/output/large_78.pt\"\n",
    "\n",
    "# for checkpoint\n",
    "# model = Model(cfg).load_from_checkpoint(checkpoint_path=ckpt_path)\n",
    "\n",
    "# for pt\n",
    "model = Model(cfg)\n",
    "model.load_state_dict(torch.load(pt_path))\n",
    "\n",
    "# gpu가 없으면 'gpus=0'을, gpu가 여러개면 'gpus=4'처럼 사용하실 gpu의 개수를 입력해주세요\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=cfg.train.max_epoch,\n",
    "    log_every_n_steps=cfg.train.logging_step,\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "pmodel = transformers.pipeline('question-answering')\n",
    "\n",
    "# define two predictions, one that outputs the logits for the range start,\n",
    "# and the other for the range end\n",
    "def f(questions, start):\n",
    "    outs = []\n",
    "    for q in questions:\n",
    "        question, context = q.split(\"[SEP]\")\n",
    "        d = pmodel.tokenizer(question, context)\n",
    "        out = pmodel.model.forward(**{k: torch.tensor(d[k]).reshape(1, -1) for k in d})\n",
    "        logits = out.start_logits if start else out.end_logits\n",
    "        outs.append(logits.reshape(-1).detach().numpy())\n",
    "    return outs\n",
    "def f_start(questions):\n",
    "    return f(questions, True)\n",
    "def f_end(questions):\n",
    "    return f(questions, False)\n",
    "\n",
    "\n",
    "data = [\"테이블 위에 무엇이 있나요?[SEP]내가 집에 돌아왔을 때 나의 고양이가 테이블 위에 있는 것을 보았고, 개구리가 바닥에 있는 것을 보았다\"]\n",
    "f_answers = make_answer_scorer([\"나의 고양이\", \"고양이\", \"나의 개구리\"])\n",
    "explainer_answers = shap.Explainer(f_answers, pmodel.tokenizer)\n",
    "shap_values_answers = explainer_answers(data)\n",
    "\n",
    "shap.plots.text(shap_values_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6433bde22504cbf34326cab27df20b94e196fcf98213f776ce9807cc95ec7583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
